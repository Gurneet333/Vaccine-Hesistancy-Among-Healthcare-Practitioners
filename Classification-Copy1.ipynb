{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1cDIDcaDWg7J"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8yeoN_fLWrrR",
    "outputId": "62bcbc10-ef54-4ffe-e6ea-16d8c07a1f1c"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-50a304b61997>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Mounting the colab on the Google drive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Mounting the colab on the Google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggsrZRj_WuwA",
    "outputId": "57bb785a-9ad8-4502-8585-848be73cf3ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/.shortcut-targets-by-id/1uKZmpRRwciHwohqfjKcRePjwzyJ7Qyhg/Data Mining Project Research/Data\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/Data\\ Mining\\ Project\\ Research/Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "V9EpLDG-W2gQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "dfinal1 = pd.read_csv('Final_Data.csv')\n",
    "dfinal2 = dfinal1.dropna(subset = ['Overall_tag_DNS'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iFD5MasOXC_h"
   },
   "outputs": [],
   "source": [
    "dfinal3 = dfinal2[['body','Overall_tag_DNS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(dfinal3.body)\n",
    "test = pd.DataFrame(dfinal3.Overall_tag_DNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall_tag_DNS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48743</th>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48747</th>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48749</th>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48765</th>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48768</th>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15963 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Overall_tag_DNS\n",
       "256                 D\n",
       "257                 D\n",
       "258                 D\n",
       "421                 N\n",
       "477                 D\n",
       "...               ...\n",
       "48743               D\n",
       "48747               D\n",
       "48749               S\n",
       "48765               S\n",
       "48768               D\n",
       "\n",
       "[15963 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (9577, 1)\n",
      "X_test shape: (3193, 1)\n",
      "y_train shape: (9577, 1)\n",
      "y_test shape: (3193, 1)\n",
      "X_val shape: (3193, 1)\n",
      "y val shape: (3193, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, test,\n",
    "    test_size=0.2, shuffle = True, random_state = 8)\n",
    "\n",
    "# Use the same function above for the validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "    test_size=0.25, random_state= 8) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))\n",
    "print(\"X_val shape: {}\".format(X_val.shape))\n",
    "print(\"y val shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D': 0, 'N': 1, 'S': 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "X=dfinal3.drop('Overall_tag_DNS',axis=1)\n",
    "Y=le.fit_transform(dfinal3['Overall_tag_DNS'])\n",
    "\n",
    "a=dict(zip(le.inverse_transform([0,1,2]),[0,1,2]))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "y_test['Overall_tag_DNS']=y_test['Overall_tag_DNS'].map(a)\n",
    "y_train['Overall_tag_DNS']=y_train['Overall_tag_DNS'].map(a)\n",
    "y_val['Overall_tag_DNS']=y_val['Overall_tag_DNS'].map(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5725\n",
       "2    2142\n",
       "1    1710\n",
       "Name: Overall_tag_DNS, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train['Overall_tag_DNS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.2\n",
      "  Downloading tensorflow-2.2.0-cp37-cp37m-win_amd64.whl (459.2 MB)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (1.1.2)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (3.19.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (1.0.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (3.3.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (1.6.3)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (1.21.2)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (1.11.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (1.14.0)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (1.4.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (1.42.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (0.34.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (0.2.0)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (1.1.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.22.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Using cached google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (45.2.0.post20200210)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.3.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.0.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.2.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in c:\\users\\hp\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.8.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in c:\\users\\hp\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.10.0.2)\n",
      "Installing collected packages: google-auth, tensorboard, tensorflow-estimator, gast, tensorflow\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.3.3\n",
      "    Uninstalling google-auth-2.3.3:\n",
      "      Successfully uninstalled google-auth-2.3.3\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.7.0\n",
      "    Uninstalling tensorboard-2.7.0:\n",
      "      Successfully uninstalled tensorboard-2.7.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.7.0\n",
      "    Uninstalling tensorflow-estimator-2.7.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.4.0\n",
      "    Uninstalling gast-0.4.0:\n",
      "      Successfully uninstalled gast-0.4.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.7.0\n",
      "    Uninstalling tensorflow-2.7.0:\n",
      "      Successfully uninstalled tensorflow-2.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\~ensorflow\\\\lite\\\\experimental\\\\microfrontend\\\\python\\\\ops\\\\_audio_microfrontend_op.so'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert\n",
    "\n",
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = X_train['body']\n",
    "y= y_train['Overall_tag_DNS']\n",
    "\n",
    "def tokenize_reviews(text_reviews):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_reviews))\n",
    "\n",
    "tokenized_reviews = [tokenize_reviews(review) for review in reviews]\n",
    "\n",
    "y = np.array(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_with_len = [[review, y[i], len(review)] for i, review in enumerate(tokenized_reviews)]\n",
    "reviews_with_len.sort(key=lambda x: x[2])\n",
    "BATCH_SIZE = 64\n",
    "sorted_reviews_labels = [(review_lab[0], review_lab[1], review_lab[2]) for review_lab in reviews_with_len]\n",
    "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_reviews_labels, output_types=(tf.int32, tf.int32,tf.int32))\n",
    "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ),(), ()))\n",
    "OUTPUT_CLASSES = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEXT_MODEL(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocabulary_size,\n",
    "                 embedding_dimensions=128,\n",
    "                 cnn_filters=50,\n",
    "                 dnn_units=512,\n",
    "                 model_output_classes=3,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"text_model\"):\n",
    "        super(TEXT_MODEL, self).__init__(name=name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocabulary_size,\n",
    "                                          embedding_dimensions)\n",
    "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=2,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=3,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=4,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if model_output_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=model_output_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        l = self.embedding(inputs)\n",
    "        l_1 = self.cnn_layer1(l) \n",
    "        l_1 = self.pool(l_1) \n",
    "        l_2 = self.cnn_layer2(l) \n",
    "        l_2 = self.pool(l_2)\n",
    "        l_3 = self.cnn_layer3(l)\n",
    "        l_3 = self.pool(l_3) \n",
    "        \n",
    "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
    "        concatenated = self.dense_1(concatenated)\n",
    "        concatenated = self.dropout(concatenated, training)\n",
    "        model_output = self.last_dense(concatenated)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "OUTPUT_CLASSES = 3\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "NB_EPOCHS = 4\n",
    "\n",
    "text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
    "                        embedding_dimensions=EMB_DIM,\n",
    "                        cnn_filters=CNN_FILTERS,\n",
    "                        dnn_units=DNN_UNITS,\n",
    "                        model_output_classes=OUTPUT_CLASSES,\n",
    "                        dropout_rate=DROPOUT_RATE)\n",
    "if OUTPUT_CLASSES == 2:\n",
    "    text_model.compile(loss=\"binary_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"accuracy\"])\n",
    "else:\n",
    "    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": " Failed to call dnnl_sgemm. Error code: 2\n\t [[node gradient_tape/text_model/conv1d_2/Conv1D/Conv2DBackpropInput\n (defined at C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:464)\n]] [Op:__inference_train_function_22957]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node gradient_tape/text_model/conv1d_2/Conv1D/Conv2DBackpropInput:\nIn[0] gradient_tape/text_model/conv1d_2/Conv1D/ShapeN:\t\nIn[1] text_model/conv1d_2/Conv1D/ExpandDims_1 (defined at C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\layers\\convolutional.py:238)\t\nIn[2] gradient_tape/text_model/conv1d_2/Conv1D/Reshape:\n\nOperation defined at: (most recent call last)\n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n>>>     \"__main__\", mod_spec)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 583, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 153, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\asyncio\\base_events.py\", line 538, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\asyncio\\base_events.py\", line 1782, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n>>>     lambda f: self._run_callback(functools.partial(callback, future))\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n>>>     ret = callback()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n>>>     self.run()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n>>>     yielded = self.gen.send(value)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 361, in process_one\n>>>     yield gen.maybe_future(dispatch(*args))\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n>>>     yielded = next(result)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n>>>     yield gen.maybe_future(handler(stream, idents, msg))\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n>>>     yielded = next(result)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 541, in execute_request\n>>>     user_expressions, allow_stdin,\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n>>>     yielded = next(result)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2858, in run_cell\n>>>     raw_cell, store_history, silent, shell_futures)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2886, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3063, in run_cell_async\n>>>     interactivity=interactivity, compiler=compiler, result=result)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3254, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"<ipython-input-18-ab85ee814ecb>\", line 1, in <module>\n>>>     text_model.fit(batched_dataset, epochs=NB_EPOCHS)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 816, in train_step\n>>>     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 531, in minimize\n>>>     loss, var_list=var_list, grad_loss=grad_loss, tape=tape)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 583, in _compute_gradients\n>>>     grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 464, in _get_gradients\n>>>     grads = tape.gradient(loss, var_list, grad_loss)\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-ab85ee814ecb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatched_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNB_EPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 59\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m:  Failed to call dnnl_sgemm. Error code: 2\n\t [[node gradient_tape/text_model/conv1d_2/Conv1D/Conv2DBackpropInput\n (defined at C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:464)\n]] [Op:__inference_train_function_22957]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node gradient_tape/text_model/conv1d_2/Conv1D/Conv2DBackpropInput:\nIn[0] gradient_tape/text_model/conv1d_2/Conv1D/ShapeN:\t\nIn[1] text_model/conv1d_2/Conv1D/ExpandDims_1 (defined at C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\layers\\convolutional.py:238)\t\nIn[2] gradient_tape/text_model/conv1d_2/Conv1D/Reshape:\n\nOperation defined at: (most recent call last)\n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n>>>     \"__main__\", mod_spec)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 583, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 153, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\asyncio\\base_events.py\", line 538, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\asyncio\\base_events.py\", line 1782, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n>>>     lambda f: self._run_callback(functools.partial(callback, future))\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n>>>     ret = callback()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n>>>     self.run()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n>>>     yielded = self.gen.send(value)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 361, in process_one\n>>>     yield gen.maybe_future(dispatch(*args))\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n>>>     yielded = next(result)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n>>>     yield gen.maybe_future(handler(stream, idents, msg))\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n>>>     yielded = next(result)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 541, in execute_request\n>>>     user_expressions, allow_stdin,\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n>>>     yielded = next(result)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2858, in run_cell\n>>>     raw_cell, store_history, silent, shell_futures)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2886, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3063, in run_cell_async\n>>>     interactivity=interactivity, compiler=compiler, result=result)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3254, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"<ipython-input-18-ab85ee814ecb>\", line 1, in <module>\n>>>     text_model.fit(batched_dataset, epochs=NB_EPOCHS)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 816, in train_step\n>>>     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 531, in minimize\n>>>     loss, var_list=var_list, grad_loss=grad_loss, tape=tape)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 583, in _compute_gradients\n>>>     grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 464, in _get_gradients\n>>>     grads = tape.gradient(loss, var_list, grad_loss)\n>>> "
     ]
    }
   ],
   "source": [
    "text_model.fit(batched_dataset, epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hQ1n_eJwXHWg"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "X=dfinal3.drop('Overall_tag_DNS',axis=1)\n",
    "Y=le.fit_transform(dfinal3['Overall_tag_DNS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eT-4IMcRXLbg",
    "outputId": "e524f5ac-1efe-4d64-a89d-842e55793be7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D': 0, 'N': 1, 'S': 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=dict(zip(le.inverse_transform([0,1,2]),[0,1,2]))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BU7CLRHPXOWB",
    "outputId": "765da2a4-3896-4016-81ce-c6061e8cee5b"
   },
   "outputs": [],
   "source": [
    "dfinal3['Overall_tag_DNS']=dfinal3['Overall_tag_DNS'].map(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YT86bLLpXSFB",
    "outputId": "8b23bb2e-c1a0-45f3-d96b-8eaa19524948"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-for-tf2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.14.9)\n",
      "Requirement already satisfied: py-params>=0.9.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from bert-for-tf2) (0.10.2)\n",
      "Requirement already satisfied: params-flow>=0.8.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from bert-for-tf2) (0.8.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.42.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.21.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.1.96)\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.21.2)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (3.10.0.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (0.22.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (12.0.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (45.2.0.post20200210)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.3.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in c:\\users\\hp\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2019.11.28)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_hub in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow_hub) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow_hub) (1.21.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "N3dE6pHUXXAR"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "IaMjGOuIXa6x"
   },
   "outputs": [],
   "source": [
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "id": "6-34c07hXdzy"
   },
   "outputs": [],
   "source": [
    "# reviews = dfinal3['body']\n",
    "# y= dfinal3['Overall_tag_DNS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "id": "EYDRLp8fXqdU"
   },
   "outputs": [],
   "source": [
    "def tokenize_reviews(text_reviews):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "id": "glbnDzlJXrLj"
   },
   "outputs": [],
   "source": [
    "tokenized_reviews = [tokenize_reviews(review) for review in reviews]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "id": "IuIuMM-0XvNz"
   },
   "outputs": [],
   "source": [
    "\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "id": "pwAY8I72X1_D"
   },
   "outputs": [],
   "source": [
    "reviews_with_len = [[review, y[i], len(review)] for i, review in enumerate(tokenized_reviews)]\n",
    "# max_size = 2402\n",
    "# reviews_with_len = [[[review[cnt] if cnt < len(review) else 0 for cnt in range(max_size)], y[i], len(review)]\n",
    "# for i, review in enumerate(tokenized_reviews)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "id": "mloWPahwX2jk"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(reviews_with_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "id": "Z0_ihQexX6eF"
   },
   "outputs": [],
   "source": [
    "reviews_with_len.sort(key=lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "id": "uvDxwVb9dcAX"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "\n",
    "sorted_reviews_labels = [(review_lab[0], review_lab[1], review_lab[2]) for review_lab in reviews_with_len]\n",
    "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_reviews_labels, output_types=(tf.int32, tf.int32,tf.int32))\n",
    "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ),(), ()))\n",
    "OUTPUT_CLASSES = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "id": "IZ0wuQP6YLlH"
   },
   "outputs": [],
   "source": [
    "# sorted_reviews_labels = [(review_lab[0], review_lab[1]) for review_lab in reviews_with_len]\n",
    "#sorted_reviews_labels = [(review_lab[0], review_lab[1], review_lab[2]) for review_lab in reviews_with_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "id": "8G9a2droYQQX"
   },
   "outputs": [],
   "source": [
    "#processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_reviews_labels, output_types=(tf.int32, tf.int32))\n",
    "#processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_reviews_labels, output_types=(tf.int32, tf.int32,tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "id": "mDUPCUjTYTcH"
   },
   "outputs": [],
   "source": [
    "#batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))\n",
    "\n",
    "#batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ),(), ()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IBpgy8m1YWdo",
    "outputId": "1463daf9-b1ac-4498-c39b-68e5951247a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(50, 2), dtype=int32, numpy=\n",
       " array([[ 6149,     0],\n",
       "        [ 2959,     0],\n",
       "        [ 2748,     0],\n",
       "        [ 1029,     0],\n",
       "        [ 3599,     0],\n",
       "        [ 2748,     0],\n",
       "        [ 2053,     0],\n",
       "        [ 2206,     0],\n",
       "        [27937,     0],\n",
       "        [ 7078,     0],\n",
       "        [  100,     0],\n",
       "        [15624,     0],\n",
       "        [ 4283,     0],\n",
       "        [10392,     0],\n",
       "        [ 6149,     0],\n",
       "        [ 2053,     0],\n",
       "        [ 6149,     0],\n",
       "        [ 4283,     0],\n",
       "        [ 2748,     0],\n",
       "        [ 2771,     0],\n",
       "        [  100,     0],\n",
       "        [ 2748,     0],\n",
       "        [ 2053,     0],\n",
       "        [ 2748,     0],\n",
       "        [ 2053,     0],\n",
       "        [ 3530,     0],\n",
       "        [ 3658,     0],\n",
       "        [ 2053,     0],\n",
       "        [ 2053,     0],\n",
       "        [ 2053,  1012],\n",
       "        [ 2053,  1012],\n",
       "        [ 2748,  1012],\n",
       "        [ 4283,   999],\n",
       "        [ 2003,  8257],\n",
       "        [17404,  1029],\n",
       "        [ 2589,  1012],\n",
       "        [ 5262,  1012],\n",
       "        [ 9616,  1029],\n",
       "        [ 6429,   999],\n",
       "        [ 2117,   999],\n",
       "        [ 2168,  1012],\n",
       "        [16215,  2595],\n",
       "        [ 2168, 14625],\n",
       "        [16780,  1012],\n",
       "        [ 2204, 28516],\n",
       "        [ 2204,  2935],\n",
       "        [ 2204, 28516],\n",
       "        [ 2033,  2205],\n",
       "        [ 2025,  7175],\n",
       "        [ 3120,  1029]])>,\n",
       " <tf.Tensor: shape=(50,), dtype=int32, numpy=\n",
       " array([1, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 1, 0, 1, 2, 2, 2, 2, 0,\n",
       "        1, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2,\n",
       "        0, 2, 2, 0, 2, 2])>,\n",
       " <tf.Tensor: shape=(50,), dtype=int32, numpy=\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2])>)"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(batched_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dataset_partitions_tf(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "    \n",
    "    if shuffle:\n",
    "        # Specify seed to always have the same split distribution between runs\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    #train_ds = ds.take(train_size)  \n",
    "    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "id": "v3fdTg8RYZmX"
   },
   "outputs": [],
   "source": [
    "# import math\n",
    "# TOTAL_BATCHES = math.ceil(len(sorted_reviews_labels) / BATCH_SIZE)\n",
    "# TEST_BATCHES = TOTAL_BATCHES // 10\n",
    "# batched_dataset.shuffle(TOTAL_BATCHES)\n",
    "\n",
    "# test_data = batched_dataset.skip(TOTAL_BATCHES -TEST_BATCHES)\n",
    "# train_data = batched_dataset.skip(TEST_BATCHES)\n",
    "\n",
    "\n",
    "#train_data, validation_data, test_data =get_dataset_partitions_tf(batched_dataset,260)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "id": "4eA6wv6AYdzX"
   },
   "outputs": [],
   "source": [
    "class TEXT_MODEL(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocabulary_size,\n",
    "                 embedding_dimensions=128,\n",
    "                 cnn_filters=50,\n",
    "                 dnn_units=512,\n",
    "                 model_output_classes=3,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"text_model\"):\n",
    "        super(TEXT_MODEL, self).__init__(name=name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocabulary_size,\n",
    "                                          embedding_dimensions)\n",
    "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=2,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=3,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=4,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if model_output_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=model_output_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        l = self.embedding(inputs)\n",
    "        l_1 = self.cnn_layer1(l) \n",
    "        l_1 = self.pool(l_1) \n",
    "        l_2 = self.cnn_layer2(l) \n",
    "        l_2 = self.pool(l_2)\n",
    "        l_3 = self.cnn_layer3(l)\n",
    "        l_3 = self.pool(l_3) \n",
    "        \n",
    "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
    "        concatenated = self.dense_1(concatenated)\n",
    "        concatenated = self.dropout(concatenated, training)\n",
    "        model_output = self.last_dense(concatenated)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "id": "JTVbqOJaYiaH"
   },
   "outputs": [],
   "source": [
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "OUTPUT_CLASSES = 3\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "NB_EPOCHS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "id": "-IbvEA1KYmUJ"
   },
   "outputs": [],
   "source": [
    "text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
    "                        embedding_dimensions=EMB_DIM,\n",
    "                        cnn_filters=CNN_FILTERS,\n",
    "                        dnn_units=DNN_UNITS,\n",
    "                        model_output_classes=OUTPUT_CLASSES,\n",
    "                        dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "id": "G5RratttYrAZ"
   },
   "outputs": [],
   "source": [
    "if OUTPUT_CLASSES == 2:\n",
    "    text_model.compile(loss=\"binary_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"accuracy\"])\n",
    "else:\n",
    "    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "id": "BCu_cWvQZ21Q"
   },
   "outputs": [],
   "source": [
    "# sorted_reviews_labels = [(review_lab[0], review_lab[1], review_lab[2]) for review_lab in reviews_with_len]\n",
    "# processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_reviews_labels, output_types=(tf.int32, tf.int32,tf.int32))\n",
    "# batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ),(), ()))\n",
    "# OUTPUT_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mt3bIc8yYuHZ",
    "outputId": "a0be4632-d9c0-4617-b942-236187681c17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "    116/Unknown - 20s 162ms/step - loss: 70.5040 - sparse_categorical_accuracy: 0.5876"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Computed output size would be negative: -1 [input_size: 2, effective_filter_size: 4, stride: 1]\n\t [[node text_model/conv1d_14/Conv1D\n (defined at C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\layers\\convolutional.py:238)\n]] [Op:__inference_train_function_126642]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node text_model/conv1d_14/Conv1D:\nIn[0] text_model/conv1d_14/Conv1D/ExpandDims:\t\nIn[1] text_model/conv1d_14/Conv1D/ExpandDims_1:\n\nOperation defined at: (most recent call last)\n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n>>>     \"__main__\", mod_spec)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 583, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 153, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\asyncio\\base_events.py\", line 538, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\asyncio\\base_events.py\", line 1782, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n>>>     lambda f: self._run_callback(functools.partial(callback, future))\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n>>>     ret = callback()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n>>>     self.run()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n>>>     yielded = self.gen.send(value)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 361, in process_one\n>>>     yield gen.maybe_future(dispatch(*args))\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n>>>     yielded = next(result)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n>>>     yield gen.maybe_future(handler(stream, idents, msg))\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n>>>     yielded = next(result)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 541, in execute_request\n>>>     user_expressions, allow_stdin,\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n>>>     yielded = next(result)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2858, in run_cell\n>>>     raw_cell, store_history, silent, shell_futures)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2886, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3063, in run_cell_async\n>>>     interactivity=interactivity, compiler=compiler, result=result)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3254, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"<ipython-input-382-1896a0dd0e98>\", line 1, in <module>\n>>>     text_model.fit(train_data, epochs=NB_EPOCHS)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n>>>     y_pred = self(x, training=True)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"<ipython-input-268-39716bdc6dca>\", line 45, in call\n>>>     l_3 = self.cnn_layer3(l)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 246, in call\n>>>     outputs = self.convolution_op(inputs, self.kernel)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 238, in convolution_op\n>>>     name=self.__class__.__name__)\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-384-1896a0dd0e98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNB_EPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 59\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  Computed output size would be negative: -1 [input_size: 2, effective_filter_size: 4, stride: 1]\n\t [[node text_model/conv1d_14/Conv1D\n (defined at C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\layers\\convolutional.py:238)\n]] [Op:__inference_train_function_126642]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node text_model/conv1d_14/Conv1D:\nIn[0] text_model/conv1d_14/Conv1D/ExpandDims:\t\nIn[1] text_model/conv1d_14/Conv1D/ExpandDims_1:\n\nOperation defined at: (most recent call last)\n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n>>>     \"__main__\", mod_spec)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 583, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 153, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\asyncio\\base_events.py\", line 538, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\asyncio\\base_events.py\", line 1782, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n>>>     lambda f: self._run_callback(functools.partial(callback, future))\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n>>>     ret = callback()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n>>>     self.run()\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n>>>     yielded = self.gen.send(value)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 361, in process_one\n>>>     yield gen.maybe_future(dispatch(*args))\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n>>>     yielded = next(result)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n>>>     yield gen.maybe_future(handler(stream, idents, msg))\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n>>>     yielded = next(result)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 541, in execute_request\n>>>     user_expressions, allow_stdin,\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n>>>     yielded = next(result)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2858, in run_cell\n>>>     raw_cell, store_history, silent, shell_futures)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2886, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3063, in run_cell_async\n>>>     interactivity=interactivity, compiler=compiler, result=result)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3254, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"<ipython-input-382-1896a0dd0e98>\", line 1, in <module>\n>>>     text_model.fit(train_data, epochs=NB_EPOCHS)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n>>>     y_pred = self(x, training=True)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"<ipython-input-268-39716bdc6dca>\", line 45, in call\n>>>     l_3 = self.cnn_layer3(l)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 246, in call\n>>>     outputs = self.convolution_op(inputs, self.kernel)\n>>> \n>>>   File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 238, in convolution_op\n>>>     name=self.__class__.__name__)\n>>> "
     ]
    }
   ],
   "source": [
    "text_model.fit(train_data, epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bkZeNDrtWgLi"
   },
   "outputs": [],
   "source": [
    "\n",
    "text_model1 = tf.keras.models.load_model('./saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oxo4GepyWga7",
    "outputId": "1c50d47c-7cb5-4800-ed63-ab58e94e8b60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 2s 30ms/step - loss: 0.0433 - sparse_categorical_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "results = text_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "id": "xS4-AJzlW8pt",
    "outputId": "fdd06085-e5fc-427f-bcfb-aa87e7a3455f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6039ea518040>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_model1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "text_model.predict(train_data,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NeInfc9MYyN5",
    "outputId": "f5e39f5e-6360-4cba-fbcd-64eb53dc5b03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 8s 101ms/step - loss: 0.0324 - sparse_categorical_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "results = text_model.evaluate(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWy3myDTDPvs",
    "outputId": "b56ad7a9-094b-4635-c7d1-5b80ef3ba22a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (3.13)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyyaml h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IIK9CsBpC9X4",
    "outputId": "4505bccb-f1da-4cc2-bb3d-c07125b2a76b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "#tf.saved_model.save(text_model,export_dir = './saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sMg-w1HLIGbD",
    "outputId": "cb225c57-a633-43b4-f393-1edc0d4d43aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "export_dir = './saved_model'\n",
    "tf.keras.models.save_model(text_model,filepath = export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "LrVfMGQBEFVW"
   },
   "outputs": [],
   "source": [
    "\n",
    "text_model1 = tf.keras.models.load_model('./saved_model')\n",
    "#text_model1  =tf.saved_model.load(export_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s5YlxDIoE6CD",
    "outputId": "3fb2ce6d-cfed-4eab-abdd-1b7f8ddae412"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.saving.saved_model.load.TEXT_MODEL at 0x1a3a66c2788>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7eK31CmBE9fG",
    "outputId": "90307d93-0d3e-416f-ba60-3b98b10e4ae0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'zmq.backend.cython.message.Frame.__dealloc__'\n",
      "Traceback (most recent call last):\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 13, in zmq.backend.cython.checkrc._check_rc\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.TEXT_MODEL at 0x7f1a65fbc050>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1594iF7Fb3fs",
    "outputId": "faf3a04f-52ef-40cd-a989-b95da4c3bbef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/450 [==============================] - 19s 41ms/step\n"
     ]
    }
   ],
   "source": [
    "a = text_model.predict(train_data,verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "id": "G5H9IUFyVoKL",
    "outputId": "98842142-1191-4460-e0b3-5d8ba36d4a35"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-64fb0553446a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_model1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'text_model1' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "9odyYIEE0MDD"
   },
   "outputs": [],
   "source": [
    "l1=[]\n",
    "for i in a:\n",
    "    l1.append(np.argmax(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uAYNzYb46D1R",
    "outputId": "1ef7d1fe-e3f7-423a-93df-f85a76e49598"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.62385190e-04, 9.99019623e-01, 4.18011303e-04],\n",
       "       [3.93844880e-02, 9.19233799e-01, 4.13817056e-02],\n",
       "       [9.86874774e-02, 2.04498202e-01, 6.96814299e-01],\n",
       "       ...,\n",
       "       [1.00000000e+00, 4.98861751e-11, 3.07547268e-08],\n",
       "       [1.00000000e+00, 1.36769086e-11, 5.86125459e-08],\n",
       "       [1.00000000e+00, 1.82745789e-12, 1.56455360e-09]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSHu9L-J6ab_",
    "outputId": "289b7abe-0219-4e50-b2ce-87f0c660baf9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25Lr7iaR6iL_",
    "outputId": "3f204b45-2c8b-4a2b-c461-dbcd9a5cfb9f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dfinal_na =dfinal1[dfinal1.Overall_tag_DNS.isna()]\n",
    "dfinal_na['Overall_tag_DNS'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "uXtAocCCXeHu",
    "outputId": "d515178b-bc46-4635-c108-e615e93f3a5f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>Index1</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>Comment_or_submission</th>\n",
       "      <th>Overall_tag_DNS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lillygirl0528</td>\n",
       "      <td>i thought this was a conspiracy group? why is ...</td>\n",
       "      <td>143.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.636751e+09</td>\n",
       "      <td>hkdo3du</td>\n",
       "      <td>C</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ktyn</td>\n",
       "      <td>I got Moderna.\\n\\nFirst dose: sore arm for a f...</td>\n",
       "      <td>274.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.627998e+09</td>\n",
       "      <td>h7jvuhf</td>\n",
       "      <td>C</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ktyn</td>\n",
       "      <td>The company I work for is also giving $100 to ...</td>\n",
       "      <td>347.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.620936e+09</td>\n",
       "      <td>gy0o0uz</td>\n",
       "      <td>C</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ktyn</td>\n",
       "      <td>Iâ€™ve had the same thoughts, glad to see Iâ€™m no...</td>\n",
       "      <td>380.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.616941e+09</td>\n",
       "      <td>gskvnfr</td>\n",
       "      <td>C</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dvncan90</td>\n",
       "      <td>90 days with or without vaccination? I don't h...</td>\n",
       "      <td>622.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.632317e+09</td>\n",
       "      <td>hdubc6u</td>\n",
       "      <td>C</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48774</th>\n",
       "      <td>iamapremed</td>\n",
       "      <td>Hi everyone!\\n\\nSo I'm working on reapplicatio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.590077e+09</td>\n",
       "      <td>gnzf48</td>\n",
       "      <td>S</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48775</th>\n",
       "      <td>dontwannabeabadger</td>\n",
       "      <td>Hi all,\\n\\nI am a biology major at a big ten s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.588386e+09</td>\n",
       "      <td>gby1j6</td>\n",
       "      <td>S</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48776</th>\n",
       "      <td>ubcthrowaway-01</td>\n",
       "      <td>I have a 4.5 gpa on a 4.33 (I got over 100% in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.625639e+09</td>\n",
       "      <td>ofd778</td>\n",
       "      <td>S</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48777</th>\n",
       "      <td>taelea</td>\n",
       "      <td>Hi all, \\n\\nSo I have my UBC interview coming ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.612451e+09</td>\n",
       "      <td>lchidk</td>\n",
       "      <td>S</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48778</th>\n",
       "      <td>SmartTrashPanda</td>\n",
       "      <td>Hey everyone! Since the new cycle is coming up...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.583558e+09</td>\n",
       "      <td>feqj95</td>\n",
       "      <td>S</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32816 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   author                                               body  \\\n",
       "0           lillygirl0528  i thought this was a conspiracy group? why is ...   \n",
       "1                    ktyn  I got Moderna.\\n\\nFirst dose: sore arm for a f...   \n",
       "2                    ktyn  The company I work for is also giving $100 to ...   \n",
       "3                    ktyn  Iâ€™ve had the same thoughts, glad to see Iâ€™m no...   \n",
       "4                dvncan90  90 days with or without vaccination? I don't h...   \n",
       "...                   ...                                                ...   \n",
       "48774          iamapremed  Hi everyone!\\n\\nSo I'm working on reapplicatio...   \n",
       "48775  dontwannabeabadger  Hi all,\\n\\nI am a biology major at a big ten s...   \n",
       "48776     ubcthrowaway-01  I have a 4.5 gpa on a 4.33 (I got over 100% in...   \n",
       "48777              taelea  Hi all, \\n\\nSo I have my UBC interview coming ...   \n",
       "48778     SmartTrashPanda  Hey everyone! Since the new cycle is coming up...   \n",
       "\n",
       "       Index1 author_flair_text   created_utc       id Comment_or_submission  \\\n",
       "0       143.0               NaN  1.636751e+09  hkdo3du                     C   \n",
       "1       274.0               NaN  1.627998e+09  h7jvuhf                     C   \n",
       "2       347.0               NaN  1.620936e+09  gy0o0uz                     C   \n",
       "3       380.0               NaN  1.616941e+09  gskvnfr                     C   \n",
       "4       622.0               NaN  1.632317e+09  hdubc6u                     C   \n",
       "...       ...               ...           ...      ...                   ...   \n",
       "48774     NaN               NaN  1.590077e+09   gnzf48                     S   \n",
       "48775     NaN               NaN  1.588386e+09   gby1j6                     S   \n",
       "48776     NaN               NaN  1.625639e+09   ofd778                     S   \n",
       "48777     NaN               NaN  1.612451e+09   lchidk                     S   \n",
       "48778     NaN               NaN  1.583558e+09   feqj95                     S   \n",
       "\n",
       "       Overall_tag_DNS  \n",
       "0                    4  \n",
       "1                    4  \n",
       "2                    4  \n",
       "3                    4  \n",
       "4                    4  \n",
       "...                ...  \n",
       "48774                4  \n",
       "48775                4  \n",
       "48776                4  \n",
       "48777                4  \n",
       "48778                4  \n",
       "\n",
       "[32816 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfinal_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93gV8Lyn7mup",
    "outputId": "c3c800de-8bd7-4664-d00d-750c20ce5817"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32816, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfinal_na.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sJ6upXOsYNwk"
   },
   "outputs": [],
   "source": [
    "dfinal_na_full=dfinal_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "DHV02Vr3YWMM"
   },
   "outputs": [],
   "source": [
    "dfinal_na=dfinal_na_full[32000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "5_wWsbDU7pRe"
   },
   "outputs": [],
   "source": [
    "tokenized_reviews = [tokenize_reviews(review) for review in dfinal_na['body']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "Ijd8d22AX2CD"
   },
   "outputs": [],
   "source": [
    "y = np.array(dfinal_na['Overall_tag_DNS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "FDOMnIcF8EPc"
   },
   "outputs": [],
   "source": [
    "reviews_with_len = [[review, y[i],len(review)] for i, review in enumerate(tokenized_reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "0kiS4WEs818w"
   },
   "outputs": [],
   "source": [
    "reviews_with_len.sort(key=lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hGr5W5UD9c8e",
    "outputId": "273ef3b3-0ada-48de-efcb-237826ba0b13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "816"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_with_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "K1O9Jbh19ARy"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "sorted_reviews_labels = [(review_lab[0], review_lab[1],review_lab[2]) for review_lab in reviews_with_len]\n",
    "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_reviews_labels, output_types=(tf.int32,tf.int32,tf.int32))\n",
    "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), (),()))\n",
    "#OUTPUT_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "id": "P37Q5QeQ-xLE"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "TOTAL_BATCHES = math.ceil(len(sorted_reviews_labels) / BATCH_SIZE)\n",
    "TEST_BATCHES = TOTAL_BATCHES // 10\n",
    "\n",
    "#test_data = batched_dataset.skip(TOTAL_BATCHES -TEST_BATCHES)\n",
    "#train_data = batched_dataset.skip(TEST_BATCHES)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5PocODOoFamg",
    "outputId": "3a111737-7c92-4b08-b8af-7068440da4c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 1s 28ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a= text_model1.predict(batched_dataset,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "X_EIABAC9QIr"
   },
   "outputs": [],
   "source": [
    "#l = []\n",
    "for i in a:\n",
    "    l.append(np.argmax(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ALsgrx_09_7S",
    "outputId": "dd19c1fa-9a33-4125-99cf-9f93a6b9a433"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(l).to_csv(\"12000_tak.csv\",index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "Z05NSmmoZTrT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "1000\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "2000\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "3000\n",
      "32/32 [==============================] - 1s 15ms/step\n",
      "4000\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "5000\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "6000\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "7000\n",
      "32/32 [==============================] - 1s 20ms/step\n",
      "8000\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "9000\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "10000\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "11000\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "12000\n",
      "32/32 [==============================] - 1s 23ms/step\n",
      "13000\n",
      "32/32 [==============================] - 1s 15ms/step\n",
      "14000\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "15000\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "16000\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "17000\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "18000\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "19000\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "20000\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "21000\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "22000\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "23000\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "24000\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "25000\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "26000\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "27000\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "28000\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "29000\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "30000\n",
      "32/32 [==============================] - 1s 21ms/step\n",
      "31000\n",
      "32/32 [==============================] - 1s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "for i in range(0,len(dfinal_na_full)-1000,1000):\n",
    "    print(i)\n",
    "    dfinal_na=dfinal_na_full[i:i+1000]\n",
    "    tokenized_reviews = [tokenize_reviews(review) for review in dfinal_na['body']]\n",
    "    y = np.array(dfinal_na['Overall_tag_DNS'])\n",
    "    reviews_with_len = [[review, y[i],len(review)] for i, review in enumerate(tokenized_reviews)]\n",
    "    reviews_with_len.sort(key=lambda x: x[2])\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    sorted_reviews_labels = [(review_lab[0], review_lab[1],review_lab[2]) for review_lab in reviews_with_len]\n",
    "    processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_reviews_labels, output_types=(tf.int32,tf.int32,tf.int32))\n",
    "    batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), (),()))\n",
    "    #OUTPUT_CLASSES = 3\n",
    "    \n",
    "    a= text_model1.predict(batched_dataset,verbose=True)\n",
    "   \n",
    "    for i in a:\n",
    "        l.append(np.argmax(i))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32816"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 =c2=c3= 0\n",
    "for i in l:\n",
    "    \n",
    "    if(i==0):\n",
    "        c1 +=1\n",
    "    elif(i==1):\n",
    "        c2+=1\n",
    "    else:\n",
    "        c3+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11401, 14003, 7412)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1, c2,c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 3s 29ms/step\n"
     ]
    }
   ],
   "source": [
    "test_pred_res = []\n",
    "test_pred= text_model1.predict(test_data,verbose=True)   \n",
    "for i in test_pred:\n",
    "    test_pred_res.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1563"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_pred_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_pred = []\n",
    "for i in test_data:\n",
    "    comb_pred.append(list(i[1].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_actual = [item for sublist in comb_pred for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1563"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter =0\n",
    "for i in range(len(test_actual)):\n",
    "    if(test_actual[i] == test_pred_res[i]):\n",
    "        counter+=1\n",
    "acc = counter / len(test_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 =c2=c3= 0\n",
    "for i in test_actual:\n",
    "    \n",
    "    if(i==0):\n",
    "        c1 +=1\n",
    "    elif(i==1):\n",
    "        c2+=1\n",
    "    else:\n",
    "        c3+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1161, 169, 233)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1,c2,c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tag = np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dfinal1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df1['Overall_tag_DNS'][dfinal1.Overall_tag_DNS.isna()] = final_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>Index1</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>Comment_or_submission</th>\n",
       "      <th>Overall_tag_DNS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lillygirl0528</td>\n",
       "      <td>i thought this was a conspiracy group? why is ...</td>\n",
       "      <td>143.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.636751e+09</td>\n",
       "      <td>hkdo3du</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ktyn</td>\n",
       "      <td>I got Moderna.\\n\\nFirst dose: sore arm for a f...</td>\n",
       "      <td>274.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.627998e+09</td>\n",
       "      <td>h7jvuhf</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ktyn</td>\n",
       "      <td>The company I work for is also giving $100 to ...</td>\n",
       "      <td>347.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.620936e+09</td>\n",
       "      <td>gy0o0uz</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ktyn</td>\n",
       "      <td>Iâ€™ve had the same thoughts, glad to see Iâ€™m no...</td>\n",
       "      <td>380.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.616941e+09</td>\n",
       "      <td>gskvnfr</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dvncan90</td>\n",
       "      <td>90 days with or without vaccination? I don't h...</td>\n",
       "      <td>622.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.632317e+09</td>\n",
       "      <td>hdubc6u</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48774</th>\n",
       "      <td>iamapremed</td>\n",
       "      <td>Hi everyone!\\n\\nSo I'm working on reapplicatio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.590077e+09</td>\n",
       "      <td>gnzf48</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48775</th>\n",
       "      <td>dontwannabeabadger</td>\n",
       "      <td>Hi all,\\n\\nI am a biology major at a big ten s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.588386e+09</td>\n",
       "      <td>gby1j6</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48776</th>\n",
       "      <td>ubcthrowaway-01</td>\n",
       "      <td>I have a 4.5 gpa on a 4.33 (I got over 100% in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.625639e+09</td>\n",
       "      <td>ofd778</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48777</th>\n",
       "      <td>taelea</td>\n",
       "      <td>Hi all, \\n\\nSo I have my UBC interview coming ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.612451e+09</td>\n",
       "      <td>lchidk</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48778</th>\n",
       "      <td>SmartTrashPanda</td>\n",
       "      <td>Hey everyone! Since the new cycle is coming up...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.583558e+09</td>\n",
       "      <td>feqj95</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48779 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   author                                               body  \\\n",
       "0           lillygirl0528  i thought this was a conspiracy group? why is ...   \n",
       "1                    ktyn  I got Moderna.\\n\\nFirst dose: sore arm for a f...   \n",
       "2                    ktyn  The company I work for is also giving $100 to ...   \n",
       "3                    ktyn  Iâ€™ve had the same thoughts, glad to see Iâ€™m no...   \n",
       "4                dvncan90  90 days with or without vaccination? I don't h...   \n",
       "...                   ...                                                ...   \n",
       "48774          iamapremed  Hi everyone!\\n\\nSo I'm working on reapplicatio...   \n",
       "48775  dontwannabeabadger  Hi all,\\n\\nI am a biology major at a big ten s...   \n",
       "48776     ubcthrowaway-01  I have a 4.5 gpa on a 4.33 (I got over 100% in...   \n",
       "48777              taelea  Hi all, \\n\\nSo I have my UBC interview coming ...   \n",
       "48778     SmartTrashPanda  Hey everyone! Since the new cycle is coming up...   \n",
       "\n",
       "       Index1 author_flair_text   created_utc       id Comment_or_submission  \\\n",
       "0       143.0               NaN  1.636751e+09  hkdo3du                     C   \n",
       "1       274.0               NaN  1.627998e+09  h7jvuhf                     C   \n",
       "2       347.0               NaN  1.620936e+09  gy0o0uz                     C   \n",
       "3       380.0               NaN  1.616941e+09  gskvnfr                     C   \n",
       "4       622.0               NaN  1.632317e+09  hdubc6u                     C   \n",
       "...       ...               ...           ...      ...                   ...   \n",
       "48774     NaN               NaN  1.590077e+09   gnzf48                     S   \n",
       "48775     NaN               NaN  1.588386e+09   gby1j6                     S   \n",
       "48776     NaN               NaN  1.625639e+09   ofd778                     S   \n",
       "48777     NaN               NaN  1.612451e+09   lchidk                     S   \n",
       "48778     NaN               NaN  1.583558e+09   feqj95                     S   \n",
       "\n",
       "      Overall_tag_DNS  \n",
       "0                   1  \n",
       "1                   1  \n",
       "2                   2  \n",
       "3                   2  \n",
       "4                   1  \n",
       "...               ...  \n",
       "48774               0  \n",
       "48775               0  \n",
       "48776               0  \n",
       "48777               0  \n",
       "48778               0  \n",
       "\n",
       "[48779 rows x 8 columns]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('Final_data_with_DNStagging.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfinal5 = dfinal2[['body','Overall_tag_DNS']][13000:]\n",
    "dfinal6=dfinal5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2963"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfinal5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfinal_full_na' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-279-4bda7077a0a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdfinal_full_na\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dfinal_full_na' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4/4 [==============================] - 0s 17ms/step\n",
      "100\n",
      "4/4 [==============================] - 0s 21ms/step\n",
      "200\n",
      "4/4 [==============================] - 0s 25ms/step\n",
      "300\n",
      "4/4 [==============================] - 0s 27ms/step\n",
      "400\n",
      "4/4 [==============================] - 0s 18ms/step\n",
      "500\n",
      "4/4 [==============================] - 0s 26ms/step\n",
      "600\n",
      "4/4 [==============================] - 0s 22ms/step\n",
      "700\n",
      "4/4 [==============================] - 0s 27ms/step\n",
      "800\n",
      "4/4 [==============================] - 0s 20ms/step\n",
      "900\n",
      "4/4 [==============================] - 0s 32ms/step\n",
      "1000\n",
      "4/4 [==============================] - 0s 20ms/step\n",
      "1100\n",
      "4/4 [==============================] - 0s 22ms/step\n",
      "1200\n",
      "4/4 [==============================] - 1s 377ms/step\n",
      "1300\n",
      "4/4 [==============================] - 0s 26ms/step\n",
      "1400\n",
      "4/4 [==============================] - 0s 29ms/step\n",
      "1500\n",
      "4/4 [==============================] - 0s 27ms/step\n",
      "1600\n",
      "4/4 [==============================] - 0s 25ms/step\n",
      "1700\n",
      "4/4 [==============================] - 0s 17ms/step\n",
      "1800\n",
      "4/4 [==============================] - 0s 27ms/step\n",
      "1900\n",
      "4/4 [==============================] - 0s 32ms/step\n",
      "2000\n",
      "4/4 [==============================] - 0s 16ms/step\n",
      "2100\n",
      "4/4 [==============================] - 0s 29ms/step\n",
      "2200\n",
      "4/4 [==============================] - 0s 17ms/step\n",
      "2300\n",
      "4/4 [==============================] - 0s 17ms/step\n",
      "2400\n",
      "4/4 [==============================] - 0s 27ms/step\n",
      "2500\n",
      "4/4 [==============================] - 0s 16ms/step\n",
      "2600\n",
      "4/4 [==============================] - 0s 23ms/step\n",
      "2700\n",
      "4/4 [==============================] - 0s 21ms/step\n",
      "2800\n",
      "4/4 [==============================] - 0s 23ms/step\n",
      "2900\n",
      "2/2 [==============================] - 0s 223ms/step\n",
      "3000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-283-790200947772>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m#OUTPUT_CLASSES = 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtext_model1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatched_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1802\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'outputs'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_outputs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1803\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mbatch_outputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1804\u001b[1;33m         raise ValueError('Unexpected result of `predict_function` '\n\u001b[0m\u001b[0;32m   1805\u001b[0m                          \u001b[1;34m'(Empty batch_outputs). Please use '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1806\u001b[0m                          \u001b[1;34m'`Model.compile(..., run_eagerly=True)`, or '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
     ]
    }
   ],
   "source": [
    "l = []\n",
    "for i in range(0,len(dfinal_na_full)-1000,100):\n",
    "    print(i)\n",
    "    dfinal_na=dfinal5[i:i+100]\n",
    "    tokenized_reviews = [tokenize_reviews(review) for review in dfinal_na['body']]\n",
    "    y = np.array(dfinal_na['Overall_tag_DNS'])\n",
    "    reviews_with_len = [[review, y[i],len(review)] for i, review in enumerate(tokenized_reviews)]\n",
    "    reviews_with_len.sort(key=lambda x: x[2])\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    sorted_reviews_labels = [(review_lab[0], review_lab[1],review_lab[2]) for review_lab in reviews_with_len]\n",
    "    processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_reviews_labels, output_types=(tf.int32,tf.int32,tf.int32))\n",
    "    batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), (),()))\n",
    "    #OUTPUT_CLASSES = 3\n",
    "    \n",
    "    a= text_model1.predict(batched_dataset,verbose=True)\n",
    "   \n",
    "    for i in a:\n",
    "        l.append(np.argmax(i))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred=l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_actual=dfinal6['Overall_tag_DNS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_actual=list(test_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-306-6e2ec823abf0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdfinal6\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Overall_tag_DNS'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdfinal6\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Overall_tag_DNS'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   4159\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4160\u001b[0m         \"\"\"\n\u001b[1;32m-> 4161\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4162\u001b[0m         return self._constructor(new_values, index=self.index).__finalize__(\n\u001b[0;32m   4163\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"map\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[1;31m# mapper is a function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter =0\n",
    "for i in range(len(test_actual)):\n",
    "    if(test_actual[i] == test_pred[i]):\n",
    "        counter+=1\n",
    "acc = counter / len(test_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4333445831927101"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>Overall_tag_DNS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36123</th>\n",
       "      <td>Thank you for these posts! Wish we learned mor...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36124</th>\n",
       "      <td>Iâ€™m pretty sure I commented last year about th...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36127</th>\n",
       "      <td>Thanks for the post!\\n\\nFor the direct real es...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36128</th>\n",
       "      <td>Thank you so much for this! Graduating school ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36130</th>\n",
       "      <td>Gosh, as a soon to be med student with the onl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48743</th>\n",
       "      <td>Hi all,\\n\\nI am a pharmacy student who applied...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48747</th>\n",
       "      <td>Saw this morning that Russia has apparently ap...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48749</th>\n",
       "      <td>I had a sore throat and mild headache today so...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48765</th>\n",
       "      <td>I just got the second dose of Pfizer and... I ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48768</th>\n",
       "      <td>Those of you here have many things on your min...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2963 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    body  Overall_tag_DNS\n",
       "36123  Thank you for these posts! Wish we learned mor...                4\n",
       "36124  Iâ€™m pretty sure I commented last year about th...                4\n",
       "36127  Thanks for the post!\\n\\nFor the direct real es...                4\n",
       "36128  Thank you so much for this! Graduating school ...                4\n",
       "36130  Gosh, as a soon to be med student with the onl...                4\n",
       "...                                                  ...              ...\n",
       "48743  Hi all,\\n\\nI am a pharmacy student who applied...                4\n",
       "48747  Saw this morning that Russia has apparently ap...                4\n",
       "48749  I had a sore throat and mild headache today so...                4\n",
       "48765  I just got the second dose of Pfizer and... I ...                4\n",
       "48768  Those of you here have many things on your min...                4\n",
       "\n",
       "[2963 rows x 2 columns]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfinal6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfinal6['Overall_tag_DNS']=dfinal6['Overall_tag_DNS'].map(a)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
